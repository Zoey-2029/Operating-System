       	     +-------------------------+
		     |			CS 140	       |
		     | PROJECT 4: FILE SYSTEMS |
		     |	   DESIGN DOCUMENT     |
		     +-------------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Ziyue Xiao <ziyuex@stanford,edu>
Bowen Tan <tanbowen@stanford.edu>
Jizhen Wang <jzwang43@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

		     INDEXED AND EXTENSIBLE FILES
		     ============================

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.


# the number of direct, indirect, and double indirect block
in a block sector
#define INODE_NUM 12
#define INODE_NUM_DIRECT 10

# the index of indirect and double indirect block in the 
blocks array
#define INDIRECT_INDEX INODE_NUM - 2
#define DOUBLE_INDIRECT_INDEX INODE_NUM - 1

# the number of sectors an indirect block and double indirect 
block can hold
#define INDIRECT_BLOCK_NUM_PER_SECTOR 128
#define DOUBLE_INDIRECT_NUM_PER_SECTOR 128 * 128


# replace the single block number with a block number array
struct inode_disk
{
  block_sector_t blocks[INODE_NUM];
}


// structure for an indirect block and a double indirect block
struct inode_indirect_sector 
{
  block_sector_t blocks[INDIRECT_BLOCK_NUM_PER_SECTOR];
};

>> A2: What is the maximum size of a file supported by your inode
>> structure?  Show your work.
Inode structure would support 10 direct blocks, 1 indirect block 
and 1 double indirect block, so the file size supported is:
Level 0 -- 10 sectors * 512 bytes per sector 
          = 5120 bytes
Level 1 -- 1 indirect block * 128 sectors * 512 bytes per sector 
          = 65536 bytes
Level 2 -- 1 doubly indirect blocks * 128 indirect blocks *
            65536 bytes per indirect block
          = 8388608 bytes
Total   -- 8459264 bytes


---- SYNCHRONIZATION ----

>> A3: Explain how your code avoids a race if two processes attempt to
>> extend a file at the same time.

When a process wants to write to a file, it will first check if current 
write would extend the file. If so, one process gets the lock of that 
inode allocate empty blocks and modify the file length. So, when two 
processes both wants to extend a file, lock of that inode would protest 
them from extending the file at the same time.

>> A4: Suppose processes A and B both have file F open, both
>> positioned at end-of-file.  If A reads and B writes F at the same
>> time, A may read all, part, or none of what B writes.  However, A
>> may not read data other than what B writes, e.g., if B writes
>> nonzero data, A is not allowed to see all zeros.  Explain how your
>> code avoids this race.

If a process tries to write to a file, it will get the lock of that 
inode, releases the lock when it finished writing the file. So, in our
design, the reader can only see all or none of what B writes. 

>> A5: Explain how your synchronization design provides "fairness".
>> File access is "fair" if readers cannot indefinitely block writers
>> or vice versa.  That is, many processes reading from a file cannot
>> prevent forever another process from writing the file, and many
>> processes writing to a file cannot prevent another process forever
>> from reading the file.

Because reading a file does not affect the inode structure, so the 
reading process will not get a lock and will not indefinitely block writers.
And writers, no matter whether they get the lock, they will not block 
readers since the reader does not need to get the lock to read files. 


---- RATIONALE ----

>> A6: Is your inode structure a multilevel index?  If so, why did you
>> choose this particular combination of direct, indirect, and doubly
>> indirect blocks?  If not, why did you choose an alternative inode
>> structure, and what advantages and disadvantages does your
>> structure have, compared to a multilevel index?

Our inode structure is a multilevel index, it contains 10 direct 
blocks, 1 indirect block and 1 double indirect block. We choose this
particular combination because it is large enough to support an 8M file. 
In fact, we can modify the static variables declared above to get a 
different combination based on our needs, only if the size of 
disk_inode preserves as a block size. 

			    SUBDIRECTORIES
			    ==============

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct thread {
  struct dir *cwd;
};
cwd is used to track the thread's current working directory

struct inode_disk {
  bool isdir;
};
This member indicates if the disk inode is a directory

---- ALGORITHMS ----

>> B2: Describe your code for traversing a user-specified path.  How
>> do traversals of absolute and relative paths differ?

Since each tokenized portion in the user path is corresponded to an
inode, we can treat each token as an individual folder/file.

We have defined "get_dir_from_path" and "get_file_name_from_path"
functions to help parse a user path.

1. The "get_dir_from_path" checks if the first char is '/'. If true,
meaning the user path is absolute, then it opens the root as "curr_dir".
Otherwise (relative path), the thread's current working dir is opened. 
Next, the function "splits" the user path by '/', and tries to lookup 
the token in "curr_dir" and extracts the inode (the lookup function is 
also updated to handle "." and ".." directory). Then it checks if the
inode is a directory and open it as "curr_dir". By doing so, the last
opened "curr_dir" is the valid directory which can be extracted from
the user path.

2. The "get_file_name_from_path" works very similar. It also "splits"
the user path by '/' and look for the last token, which is the file 
name to return.


---- SYNCHRONIZATION ----

>> B4: How do you prevent races on directory entries?  For example,
>> only one of two simultaneous attempts to remove a single file
>> should succeed, as should only one of two simultaneous attempts to
>> create a file with the same name, and so on.

We have implemented a global filesys lock. So, each sys_call will try
to acquire the lock before it can perform. In this way, each sys_call 
will be atomic so no other thread can interrupt it.

Admittedly, this may not be the best design, since there is only one 
global lock and if multiple threads are calling independent sys_calls,
they must execute one-by-one, instead of in-parallel.

Also, we had the idea to add a "dir_lock" for each opened directory. In
this way independent sys_calls can run in-parallel. If we have more time,
we will make this selection.

>> B5: Does your implementation allow a directory to be removed if it
>> is open by a process or if it is in use as a process's current
>> working directory?  If so, what happens to that process's future
>> file system operations?  If not, how do you prevent it?

It's not allowed to remove a directory which is in use or opened by 
other threads. Though we did not implement individual lock for each 
directory, we're checking the "open_cnt" of inode before removing a
directory, making sure there's no other process is using it.

---- RATIONALE ----

>> B6: Explain why you chose to represent the current directory of a
>> process the way you did.

The current working directory is tracked by the "cwd" member in the 
thread structure, and it's initialized to NULL when creating a thread. And
for the first time accessing it, it'll be set to the root. 
Also, it's easy to maintain and update. For any chdir operation, just need
to close the precious cwd and link the new opened one to it.
			     BUFFER CACHE
			     ============

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct cache_entry
{
  block_sector_t sector;
  struct list_elem elem;      /* list element for frame table */
  struct list_elem read_elem; /* list element for frame table */
  char data[BLOCK_SECTOR_SIZE];
  bool dirty;
  bool loaded;
  bool accessed;
  struct semaphore sema;
};

The cache_entry struct saves the data we need for using the cache table.

static struct list cache;
static struct list read_ahead_list; /* List of entries to read ahead. */
static int cache_size = 0;
static bool evict_cache (void);          /* LRU eviction for cache. */
static struct lock cache_lock;           /* Lock for cache table. */
static struct lock read_ahead_lock;      /* Lock for read ahead. */
static struct semaphore read_ahead_sema; /* Sema for read ahead. */

---- ALGORITHMS ----

>> C2: Describe how your cache replacement algorithm chooses a cache
>> block to evict.

We implemented the LRU algorithm with second chance. When a cache entry is 
accessed, we set the accessed bit to 1. When evicting, if the bit is 1, 
we set it to 0 and pushed to the back (second change). If access bit is 0 and
the entry is fully loaded from read_ahead or read, we can safely write to disk and 
evict the cache.

>> C3: Describe your implementation of write-behind.

As the cache table gets initialized, the program will start a 
separate thread with function write_behind.

This thread will run an infinite loop, write dirty cache to disk and 
go to sleep using timer_sleep(). 

Dirty caches are marked by the dirty 
bit in the cache_entry when a write happens the corresponding cache 
will be set to dirty, allowing write behind to write to disk.


>> C4: Describe your implementation of read-ahead.

As the cache table gets initialized, the program will start a separate 
thread that goes to sleep by default. 

The thread only wakes up if we need read ahead and sema is up on read_ahead_sema;
The read_ahead_list contains what are needed to be read ahead, the thread will
read those, the loaded to true so other threads know the cache is loaded.

After finishing reading, the thread will go back sleep by sema_down, waiting to
wake up again.

---- SYNCHRONIZATION ----

>> C5: When one process is actively reading or writing data in a
>> buffer cache block, how are other processes prevented from evicting
>> that block?

We have a cache_lock that needs to be acquired when performing
any task on the cache table. Thus, if a process is reading or writing,
other processes will wait until the action finishes before they can
possibly start evicting the buffer cache block.

>> C6: During the eviction of a block from the cache, how are other
>> processes prevented from attempting to access the block?

Eviction only happens when we try to insert a cache block,
which requires the lock on the cache table. 

Other processes will need to acquire the cache lock before 
accessing this block, and the lock is only released after the eviction is 
finished.

Thus, other processes are prevented from attempting to access the block until 
eviction is finished.

---- RATIONALE ----

>> C7: Describe a file workload likely to benefit from buffer caching,
>> and workloads likely to benefit from read-ahead and write-behind.

Buffer caching: If a file needs to be read and written multiple times 
in a short time period, the workload will benefit a lot 
since we do not have to perform many disk operations, 
just need to update the cache.

Read-ahead: when a workload is loading a large sequential file,
it is going to benefit since read-ahead allow us to load the info beforehand,
and saves time.

Write-behind: when a workload modifies certain files/sectors many times,
it is going to benefit since instead of writing immediately with many disk ops,
we write-behind when the updates are finished.


			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students in future quarters?

>> Any other comments?
